name: Datatrove Slurm Pipeline Execution

on:
  workflow_dispatch: # Allows manual triggering

jobs:
  run_datatrove_job:
    runs-on: self-hosted # Use your machine or a dedicated self-hosted runner

    steps:
    - name: 1. Checkout Slurm Cluster Repo and App Files
      uses: actions/checkout@v4
      with:
        path: slurm-docker-cluster # Clones into the subdirectory where we run commands

    - name: 2. Set Dynamic Slurm Resources
      # Define total resources available on THIS CI runner
      env:
        # Calculate per-node values (assuming 1 node, and RAM in MB)
        SLURM_HOST_CPUS: 32
        SLURM_HOST_MEMORY: 80000

      run: |
        CONF_TEMPLATE="template-slurm.conf"
        CONF_OUTPUT="slurm.conf"
        envsubst < "$CONF_TEMPLATE" > "$CONF_OUTPUT"
      working-directory: slurm-docker-cluster

    - name: 3. Build and Start Slurm Cluster
      # Define environment variables to be used by docker-compose.yml
      env:
        # Define the actual paths on the host system (CI Runner)
        HOST_INPUT_PATH: /data/collections/AWP1 #CURRENTLY ONLY 1 COLLECTION AT A TIME
        HOST_OUTPUT_PATH: /tmp/datatrove_output
        HOST_LOGS_PATH: /tmp/datatrove_logs
      # Builds the image using the modified Dockerfile (with the Conda environment)
      run: |
        docker compose build
        docker compose up -d
      working-directory: slurm-docker-cluster

    - name: 4. Wait for Slurm Daemons to Initialize
      # Give the Slurm services time to configure and register the worker nodes.
      run: sleep 30
      working-directory: slurm-docker-cluster

    - name: 5. Execute the Main Datatrove Pipeline Script
      # Define execution parameters and pass them as arguments to the Python script
      env:
        DATATROVE_TASKS: 10000          # Total tasks to process
        DATATROVE_WORKERS: 32           # Max concurrent Slurm jobs (passed to executor.workers)
        DATATROVE_CPUS_PER_TASK: 1      # CPUs per task
        DATATROVE_MEM_PER_CPU_GB: 2     # Memory used per task in GB
        INPUT_DIR: /app/data/input
        OUTPUT_DIR: /app/data/output
        LOGS_DIR: /app/data/logs

      run: |
        echo "Submitting pipeline with ${DATATROVE_TASKS} tasks and ${DATATROVE_WORKERS} workers to Slurm..."
        # Use 'docker exec' to call the Python script with all arguments
        docker exec slurmctld bash -c "export PATH='/opt/conda_envs/datatrove_env/bin:$PATH' && \
          sinfo && python /app/datatrove_pipeline.py \
          ${INPUT_DIR} \
          -t ${DATATROVE_TASKS} \
          -w ${DATATROVE_WORKERS} \
          -c ${DATATROVE_CPUS_PER_TASK} \
          -m ${DATATROVE_MEM_PER_CPU_GB} \
          -l ${LOGS_DIR} \
          -o ${OUTPUT_DIR}"
      working-directory: slurm-docker-cluster

    - name: 6. Monitor Slurm Jobs Until Completion
      # A simple loop to confirm all jobs launched by the pipeline have finished
      run: |
        echo "Monitoring Slurm Queue for job completion..."
        # We check the queue status inside the slurmctld container
        while docker exec slurmctld squeue -h | grep normal; do
          sleep 60
        done
        echo "All Slurm jobs complete. Pipeline finished."
      working-directory: slurm-docker-cluster

    - name: 7. Clean up the Slurm Cluster
      if: always() # Run cleanup even if a previous step fails
      run: docker compose down -v
      working-directory: slurm-docker-cluster